{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74f45b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(r\"documents\\2101.01158v1.pdf\")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97cfc02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0 Preview:\n",
      " A Hybrid Learner for Simultaneous Localization\n",
      "and Mapping\n",
      "Thangarajah Akilan, Member, IEEE, Edna Johnson, Japneet Sandhu, Ritika Chadha, Gaurav Taluja\n",
      "Abstractâ€”Simultaneous localization and mapping (SLAM) is\n",
      "used to predict the dynamic motion path of a moving platform\n",
      "based on the location coordinates and the precise mapping of the\n",
      "physical environment. SLAM has great potential in augmented\n",
      "reality (AR), autonomous vehicles, viz. self-driving cars, drones,\n",
      "Autonomous navigation robots (ANR). Th\n",
      "Doc 1 Preview:\n",
      " consistent map of this unknown environment and determine\n",
      "the location relative to the map. Through SLAM, robots and\n",
      "vehicles can be truly and completely automated without any\n",
      "or minimal human intervention. But the estimation of maps\n",
      "consists of various other entities, such as large storage issues,\n",
      "precise location coordinates, which makes SLAM a rather\n",
      "intriguing task, especially in the real-time domain.\n",
      "Many researches have been done worldwide to determine\n",
      "the efï¬cient method to perform SLAM. I\n",
      "Doc 2 Preview:\n",
      " C. The Front-end Feature Extractor\n",
      "As discussed earlier the PoseNet take advantage of transfer\n",
      "learning (TL), whereby it uses pretrained DCNN as feature\n",
      "extractor. TL differs from traditional learning, as, in latter, the\n",
      "models or tasks are isolated and function separately. They do\n",
      "not retain any knowledge, whereas TL learns from the older\n",
      "problem and leverages the new set of problems [10]. Thus, in\n",
      "this work, versions of ResNet, versions of VGG, and AlexNet\n",
      "are investigated. Some basic informat\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs[:3]):\n",
    "    print(f\"Doc {i} Preview:\\n\", doc.page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90cdcba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunked_docs = splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c22284db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "db = Chroma.from_documents(\n",
    "    documents=chunked_docs,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"chroma_db\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffd1fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Connect to your local LLaMA model\n",
    "llm = ChatOllama(model=\"llama3.1:8b\")\n",
    "\n",
    "# Setup conversational chain with memory\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    memory=ConversationBufferMemory(return_messages=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dd88f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  AI Assistant ready. Type 'exit' to quit.\n",
      "\n",
      "Assistant: Based on the provided context, I can describe SLAM (Simultaneous Localization and Mapping) as follows:\n",
      "\n",
      "SLAM is an algorithm that computes the relative location of a moving platform or vehicle on a pre-defined map. It has various applications in fields such as self-driving vehicles, space and maritime exploration, indoor positioning, and search and rescue operations.\n",
      "\n",
      "The primary responsibility of a SLAM algorithm is to produce an understanding of the environment and the location of the vehicle by providing its coordinates. This enables the formation of a trajectory to determine the view at a particular point in time.\n",
      "\n",
      "In essence, SLAM algorithms aim to simultaneously localize (determine the position) and map (build a representation of the environment) the surrounding area, often using sensor data such as cameras, lidar, or GPS.\n",
      "\n",
      "Assistant: According to the provided context, PoseNet is mentioned as a neural network that can be used for appearance-based SLAM. However, it doesn't provide specific information on how to use PoseNet in SLAM.\n",
      "\n",
      "To answer your question, I would recommend looking into research papers or online tutorials that demonstrate how to integrate PoseNet with SLAM algorithms. Some possible steps might include:\n",
      "\n",
      "1. Training the PoseNet model on a dataset of images collected at multiple locations.\n",
      "2. Using the trained model to predict poses (positions and orientations) for new, unseen images.\n",
      "3. Combining the predicted poses with a SLAM algorithm (such as ORB-SLAM or COLMAP) to estimate the camera's pose and reconstruct the environment.\n",
      "\n",
      "Please note that this is just a rough outline, and I don't have more specific information on how to use PoseNet in SLAM.\n",
      "\n",
      "Assistant: According to the provided context, it appears that PoseNet is a type of neural network (NN) used for appearance-based SLAM. To answer your question, I will attempt to provide some general information.\n",
      "\n",
      "To use PoseNet in SLAM, you would typically follow these steps:\n",
      "\n",
      "1. Train the PoseNet model using a set of visual samples collected at multiple discrete locations.\n",
      "2. Once trained, feed new images or frames into the PoseNet network for prediction.\n",
      "3. The network should be able to predict the camera pose (6DOF) and possibly other relevant information, such as camera intrinsics.\n",
      "\n",
      "However, it's worth noting that the details on how to use PoseNet in SLAM might require additional context or specific implementation guidelines, which are not provided here.\n",
      "\n",
      "If you need more detailed instructions or a more precise explanation, I recommend consulting the original research papers or official documentation related to PoseNet and SLAM for further guidance.\n",
      "\n",
      "Assistant: I don't have specific information on how to use PoseNet in SLAM (Simultaneous Localization and Mapping) from the provided context, but I can try to provide a more general answer.\n",
      "\n",
      "PoseNet is a deep learning-based approach for visual pose estimation that can be used as a component of SLAM systems. In general, you would need to integrate PoseNet into your SLAM framework in the following way:\n",
      "\n",
      "1. Preprocess the input images (e.g., from a camera) and pass them through PoseNet to obtain estimates of the camera's 6-DOF pose (3D translation and rotation).\n",
      "2. Use the predicted poses as input to the SLAM algorithm, which can then use this information to update the map and estimate the camera's position.\n",
      "3. The fusion of PoseNet predictions with other SLAM components (such as odometry or lidar data) can improve the overall accuracy and robustness of the system.\n",
      "\n",
      "However, without more specific details about your implementation or requirements, it's difficult to provide a more precise answer. If you have any further questions or need help with implementing PoseNet in your SLAM system, feel free to ask!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ§  AI Assistant ready. Type \u001b[39m\u001b[33m'\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to quit.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     query = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mYou: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m query.lower() \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     19\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Miniconda\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1280\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Miniconda\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "chat_history = []  # Can ignore this, memory handles it internally\n",
    "\n",
    "print(\"ðŸ§  AI Assistant ready. Type 'exit' to quit.\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"\\nYou: \")\n",
    "    if query.lower() in (\"exit\", \"quit\"):\n",
    "        break\n",
    "\n",
    "    # Only pass the question\n",
    "    result = qa_chain.invoke({\"question\": query})\n",
    "\n",
    "    print(\"\\nAssistant:\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386091e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
